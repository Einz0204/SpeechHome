import sounddevice as sd
import scipy.io.wavfile as wavfile
import librosa
import numpy as np
import tensorflow as tf
import tkinter as tk
import time
import threading
import os
from scipy.io.wavfile import write

# === è¨­å®šåƒæ•¸ ===
fs = 16000
duration = 3  # éŒ„éŸ³ç§’æ•¸
record_file = 'raw_record.wav'
trimmed_file = 'trimmed_record.wav'
model_path = os.path.join('classifier', 'cnn_model.h5')

# === è¼‰å…¥æŒ‡ä»¤æ¨™ç±¤ ===
with open("classifier/labels.txt", "r", encoding="utf-8") as f:
    COMMANDS = [line.strip() for line in f.readlines()]

# === éŒ„éŸ³å‡½å¼ ===
def record_audio():
    print("ğŸ™ï¸ æº–å‚™éŒ„éŸ³ï¼Œè«‹åœ¨ 3 ç§’å…§èªªå‡ºæŒ‡ä»¤")
    time.sleep(0)
    audio = sd.rec(int(fs * duration), samplerate=fs, channels=1, dtype='int16')
    sd.wait()
    wavfile.write(record_file, fs, audio)
    print("âœ… éŒ„éŸ³å®Œæˆ")

# === å»é™¤éœéŸ³å€ï¼Œè£å‰ªèªéŸ³é•·åº¦ ===
def trim_speech(filename, target_duration=1.5):
    y, sr = librosa.load(filename, sr=fs)
    intervals = librosa.effects.split(y, top_db=20)  # éœéŸ³åˆ†æ®µ
    if len(intervals) == 0:
        print("âš ï¸ æ²’æœ‰åµæ¸¬åˆ°èªéŸ³ï¼Œè«‹å†è©¦ä¸€æ¬¡")
        return None
    speech = np.concatenate([y[start:end] for start, end in intervals])
    target_len = int(target_duration * sr)
    if len(speech) < target_len:
        speech = np.pad(speech, (0, target_len - len(speech)))
    else:
        speech = speech[:target_len]
    return speech

# === è£œé½Š MFCC é•·åº¦ ===
def fix_mfcc_length(mfcc, target_frames=32):
    current_frames = mfcc.shape[1]
    if current_frames < target_frames:
        pad_width = target_frames - current_frames
        mfcc = np.pad(mfcc, ((0, 0), (0, pad_width)), mode='constant')
    else:
        mfcc = mfcc[:, :target_frames]
    return mfcc

# === æ¨¡å‹é æ¸¬æŒ‡ä»¤ ===
def predict_command(signal):
    mfcc = librosa.feature.mfcc(y=signal, sr=fs, n_mfcc=40)
    mfcc = fix_mfcc_length(mfcc, target_frames=32)
    mfcc = mfcc[..., np.newaxis]         # shape: (40, 32, 1)
    mfcc = np.expand_dims(mfcc, axis=0)  # shape: (1, 40, 32, 1)

    model = tf.keras.models.load_model(model_path)
    prediction = model.predict(mfcc)
    predicted_index = np.argmax(prediction)
    confidence = prediction[0][predicted_index]
    return COMMANDS[predicted_index], confidence

# === åŸ·è¡ŒéŒ„éŸ³èˆ‡é æ¸¬æµç¨‹ï¼ˆfor GUIï¼‰===
def start_process():
    threading.Thread(target=process).start()

def process():
    record_audio()
    signal = trim_speech(record_file)
    if signal is None:
        result_var.set("âš ï¸ æ²’æœ‰åµæ¸¬åˆ°èªéŸ³")
        return

    # å„²å­˜è£å‰ªå¾ŒéŸ³æª”
    write(trimmed_file, fs, (signal * 32767).astype(np.int16))
    print(f"ğŸ’¾ å·²å„²å­˜è£å‰ªå¾ŒéŸ³è¨Šï¼š{trimmed_file}")

    result, conf = predict_command(signal)
    result_text = f"ğŸ”Š æŒ‡ä»¤ï¼š{result}\nä¿¡å¿ƒå€¼ï¼š{conf:.2f}"
    print(f"\nğŸ”Š é æ¸¬çµæœï¼š{result}ï¼ˆä¿¡å¿ƒå€¼ï¼š{conf:.2f}ï¼‰")
    result_var.set(result_text)

# === GUI ä»‹é¢ ===
root = tk.Tk()
root.title("ğŸ¤ èªéŸ³è¾¨è­˜ç³»çµ±")

result_var = tk.StringVar()
label = tk.Label(root, textvariable=result_var, font=("Arial", 16), fg="blue")
label.pack(pady=20)

btn = tk.Button(root, text="ğŸ™ï¸ é–‹å§‹éŒ„éŸ³", font=("Arial", 14), command=start_process)
btn.pack(pady=10)

add_btn = tk.Button(root, text="â• æ–°å¢æŒ‡ä»¤ï¼ˆæœªå¯¦ä½œï¼‰", font=("Arial", 12))
add_btn.pack(pady=5)

root.mainloop()

